{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0KsSTqdKDGeF0+nkl31J3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":742},"id":"1AaQrRexNpFd","executionInfo":{"status":"ok","timestamp":1752876062858,"user_tz":-330,"elapsed":37355,"user":{"displayName":"Prasanna Manikandan","userId":"11061605077282949920"}},"outputId":"e60223b9-14cb-412d-e302-8aba16b2b4fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://b3aea6b69f4b13218e.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://b3aea6b69f4b13218e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":4}],"source":["# One-time installs\n","!apt install -y tesseract-ocr\n","!pip install -q pytesseract gtts pydub gradio transformers torchaudio\n","\n","# Imports\n","import pytesseract\n","from PIL import Image\n","from gtts import gTTS\n","import os\n","from pydub import AudioSegment\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","import torch\n","import gradio as gr\n","import tempfile\n","\n","# Captioning model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n","\n","def describe_image(img):\n","    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n","    out = model.generate(**inputs)\n","    return processor.decode(out[0], skip_special_tokens=True)\n","\n","def process_uploaded_images(images):\n","    clips = []\n","    log = []\n","\n","    for idx, img_path in enumerate(images, start=1):\n","        try:\n","            img = Image.open(img_path.name).convert(\"RGB\")\n","            text = pytesseract.image_to_string(img).strip()\n","\n","            if text:\n","                narration = f\"Let me speak about the image you uploaded.Wow I can read the text I see  : {text}.\"\n","            else:\n","                desc = describe_image(img)\n","                narration = f\"Let me speak the image you uploaded. I cant see any text in the image but I can see: {desc}.\"\n","\n","            tts = gTTS(text=narration, lang='en', tld='co.in')  # Indian accent\n","            tmp_mp3 = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n","            tts.save(tmp_mp3.name)\n","            clips.append(AudioSegment.from_file(tmp_mp3.name))\n","            log.append(narration)\n","\n","        except Exception as e:\n","            log.append(f\"‚ö†Ô∏è Error processing image {idx}: {e}\")\n","\n","    # Merge all clips into one\n","    if clips:\n","        final_audio = clips[0]\n","        for segment in clips[1:]:\n","            final_audio += AudioSegment.silent(duration=700) + segment\n","        final_output_path = \"/tmp/final_narration.mp3\"\n","        final_audio.export(final_output_path, format=\"mp3\")\n","        return final_output_path, \"\\n\\n\".join(log)\n","    else:\n","        return None, \"No valid images processed.\"\n","\n","# Gradio UI\n","ui = gr.Interface(\n","    fn=process_uploaded_images,\n","    inputs=gr.File(file_types=[\"image\"], label=\"Upload Multiple Images\", file_count=\"multiple\"),\n","    outputs=[\n","        gr.Audio(label=\"üó£Ô∏è Final Narration (MP3)\", type=\"filepath\"),\n","        gr.Textbox(label=\"üìù Narration Log\")\n","    ],\n","    title=\"üñºÔ∏è OCR + Image Caption to Speech\",\n","    description=\"Upload multiple images. The system will extract text or describe the image, and speak everything in Indian female voice as one MP3 narration.\"\n",")\n","\n","ui.launch()\n"]}]}